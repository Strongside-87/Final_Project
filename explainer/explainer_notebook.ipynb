{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation: \n",
    "\n",
    "- ### What is your dataset?\n",
    "    The primary dataset used in the analysis is the \"Fire Incidents\" dataset, which is available on the San Francisco Open Data Portal. It contains detailed information about fire incidents that have occurred in San Francisco from 2003 to the present. The dataset includes information such as the location, date and time of the incident, type of property affected, and details about the incident, such as the cause and the level of damage. But we have chosen to look at data from the year 2010 until 2022 to make it more manageable to visualize.\n",
    "\n",
    "    Another data set that we have used is the \"Francisco Planning Neighborhood Groups\", which provides information about the different neighborhoods in San Francisco. In addition, we have used the dataset \"City-owned Facilities - Fire and Police\", which contains various information such as name, post code, address, facility ID, etc. on police and fire stations.\n",
    "\n",
    "- ### Why did you choose this/these particular dataset(s)?\n",
    "\n",
    "    Fires are a common occurrence worldwide, and they can cause significant damage to property and lives. As such, it is essential to study and analyze fire incidents to understand their causes and how to prevent them from happening in the future. \n",
    "\n",
    "- ### What was your goal for the end user's experience?\n",
    "\n",
    "    The goal for the end user's experience with this analysis could be to gain insights into the patterns and trends of fire incidents in San Francisco, such as the frequency and causes of fires, the locations and times of fire incidents, and the effectiveness of fire response and prevention measures. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic stats. Let's understand the dataset better\n",
    "\n",
    "To dive in to analyze our dataset, we made use of variety of Python libraries for data analysis and visualization, Bokeh, NumPy, ipyleaflet, Plotly, seaborn, folium, and others. These libraries provide functions and tools for creating maps, graphs, and other visualizations of data, as well as for manipulating and analyzing data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T20:03:05.949162Z",
     "start_time": "2023-05-04T20:03:05.263318Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from bokeh.models import ColumnDataSource,Legend\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.palettes import Category10\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_file\n",
    "import numpy as np\n",
    "from ipyleaflet import Map, GeoJSON, Marker, AwesomeIcon, FullScreenControl\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from folium.plugins import HeatMap\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "import calplot\n",
    "import mpld3\n",
    "import folium\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a short section that discusses the dataset stats, containing key points/plots from your exploratory data analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write about your choices in data cleaning and preprocessing\n",
    "\n",
    "To conduct meaningful analysis and obtain valuable insights from the data we did data cleaning and preprocessing, which are essential steps before conducting any analysis on the dataset.\n",
    "The code first checks whether the dataset is available locally or not. If not, it downloads the dataset from a remote API and saves it locally for further analysis. Then, it performs several preprocessing steps, such as converting date columns to datetime format, extracting the month and year from the incident date, and filtering the dataset to include only the rows that belong to San Francisco city and between the date range of 2010 to 2022.\n",
    "\n",
    "Moreover, the code also drops unnecessary columns that are not needed for the analysis. Additionally, it creates a new column called 'focuse_Situation_by_number' by extracting the first three characters from the 'Primary Situation' column. Finally, it drops two error rows from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T20:16:12.186889Z",
     "start_time": "2023-05-02T20:15:59.722036Z"
    }
   },
   "outputs": [],
   "source": [
    "##\n",
    "#\n",
    "# Author: \n",
    "# Salim Omar\n",
    "#\n",
    "##\n",
    "\n",
    "# cleaning and preprocessing\n",
    "\n",
    "csv_path = \"../../Fire_Incidents.csv\"\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    # Download CSV from API if it doesn't exist locally\n",
    "    url = \"https://data.sfgov.org/resource/wr8u-xric.json\"\n",
    "    response = requests.get(url)\n",
    "    df = pd.read_json(response.text)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"The data has been readed from \", url)\n",
    "else:\n",
    "    # Load CSV from local file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"The data is found locally\")\n",
    "\n",
    "#df.head()\n",
    "\n",
    "\n",
    "# cleaning and preprocessing\n",
    "\n",
    "\n",
    "df['Incident Date'] = pd.to_datetime(df['Incident Date'])\n",
    "df['Alarm DtTm'] = pd.to_datetime(df['Alarm DtTm'])\n",
    "df['Arrival DtTm'] = pd.to_datetime(df['Arrival DtTm'])\n",
    "df['Close DtTm'] = pd.to_datetime(df['Close DtTm'])\n",
    "\n",
    "df['Incident Time'] = df['Incident Date'].dt.time\n",
    "df['Incident month'] = df['Incident Date'].dt.month\n",
    "df['Incident year'] = df['Incident Date'].dt.year\n",
    "\n",
    "\n",
    "# Get the data from date 2010-04-01 to 2023-04-01\n",
    "df = df[(df['Incident year'] >= 2010) &\n",
    "        (df['Incident year'] <= 2022)]\n",
    "\n",
    "\n",
    "# print the number of rows\n",
    "num_rows = df.shape[0]\n",
    "print(\"The number of rows is:\", num_rows)\n",
    "\n",
    "# get just the data from Sf city\n",
    "df = df[(df['City'] == 'SF') | (df['City'] == 'San Francisco')\n",
    "        | (df['City'] == 'SAN FRANCISCO')]\n",
    "\n",
    "# Deleting all unnaseccary columns\n",
    "df.drop(columns=['Exposure Number',\n",
    "                'Box',\n",
    "                'Fire Fatalities',\n",
    "                'Fire Injuries',\n",
    "                'Civilian Fatalities',\n",
    "                'Civilian Injuries',\n",
    "                'Number of Alarms',\n",
    "                'Mutual Aid',\n",
    "                'Action Taken Secondary',\n",
    "                'Action Taken Other',\n",
    "                'Area of Fire Origin',\n",
    "                'Ignition Cause',\n",
    "                'Ignition Factor Primary',\n",
    "                'Ignition Factor Secondary',\n",
    "                'Item First Ignited',\n",
    "                'Human Factors Associated with Ignition',\n",
    "                'Structure Type',\n",
    "                'Structure Status',\n",
    "                'Floor of Fire Origin',\n",
    "                'Fire Spread',\n",
    "                'No Flame Spead',\n",
    "                'Number of floors with minimum damage',\n",
    "                'Number of floors with significant damage',\n",
    "                'Number of floors with heavy damage',\n",
    "                'Number of floors with extreme damage',\n",
    "                'Detectors Present',\n",
    "                'Detector Type',\n",
    "                'Detector Operation',\n",
    "                'Detector Effectiveness',\n",
    "                'Detector Failure Reason',\n",
    "                'Automatic Extinguishing System Present',\n",
    "                'Automatic Extinguishing Sytem Type',\n",
    "                'Automatic Extinguishing Sytem Perfomance',\n",
    "                'Automatic Extinguishing Sytem Failure Reason',\n",
    "                'Number of Sprinkler Heads Operating'\n",
    "                ], inplace=True)\n",
    "# the code for Primary Situation\n",
    "df['focuse_Situation_by_number'] = df['Primary Situation'].str[:3]\n",
    "\n",
    "# error rows\n",
    "df.drop(df[df['ID'] == 140383810 ].index, axis=0, inplace=True)\n",
    "df.drop(df[df['ID'] == 140660390 ].index, axis=0, inplace=True)\n",
    "\n",
    "print(\"Example on dataset:\")\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code down bellow helps us to focus on the most relevant variables and clean the data, which is crucial for any data analysis. This code create a new dataframe (top10_df) that focuses on the key variables that we want to analyze: Primary Situation, focuse_Situation_by_number, neighborhood_district, and Incident year. This code also allows us to clean and preprocess the Primary Situation variable by replacing two similar categories with one common category, and removing a category that is not relevant for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#\n",
    "# Author: \n",
    "# Salim Omar\n",
    "#\n",
    "##\n",
    "# create a new dataset for top 10 Primary Situation\n",
    "top10_df = df[['Primary Situation', 'focuse_Situation_by_number','neighborhood_district','Incident year']].copy()\n",
    "\n",
    "# replace 2 coulms in 1 \n",
    "top10_df['Primary Situation'].replace(['745 Alarm system sounded/no fire-accidental',\n",
    "                                '735 Alarm system sounded due to malfunction'], '745 Alarm system activation', inplace=True)\n",
    "top10_df = top10_df[top10_df['Primary Situation'] != '554 Assist invalid']\n",
    "\n",
    "# add Situation_by_code\n",
    "top10_df['focuse_Situation_by_number'] = top10_df['Primary Situation'].str[:3]\n",
    "top10_df.head(100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code down bellow we further cleaning and preprocessing the data set.  The code replaces some values in the 'Primary Situation' column to group similar situations together, drops some null values, and splits multiple situations separated by a comma to only keep the first situation. The resulting 'call_Situation' column contains the cleaned and processed values. \n",
    "\n",
    "Then, the code calculates the count of each unique value in the 'call_Situation' column and identifies the top 10 most frequent situations using the 'value_counts()' and 'nlargest()' functions. \n",
    "\n",
    "And now we can create our plot over top 10 Primary Situations in order to see the top 10 types of calls that were registered the most.\n",
    "\n",
    "This information is useful for identifying the most common situations that the San Francisco Fire Department responds to, which can help prioritize resources and improve response strategies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#\n",
    "# Author: \n",
    "# Salim Omar\n",
    "#\n",
    "##\n",
    "\n",
    "# Becouse some of Primary Situation have '-' but it's have the same value and code so I delete \n",
    "top10_df['Primary Situation'] = top10_df['Primary Situation'].str.replace('- ', '')\n",
    "top10_df.dropna(subset=['Primary Situation'], inplace=True)\n",
    "top10_df['Primary Situation'] = top10_df['Primary Situation'].dropna().apply(\n",
    "    lambda x: x.split(',')[0])\n",
    "\n",
    "# print(call_Situation)\n",
    "call_Situation = top10_df['Primary Situation']\n",
    "len(call_Situation)\n",
    "# by using unique() fun we can se the diffrenet type of data\n",
    "\n",
    "ListOfSituation = call_Situation.unique()\n",
    "#print(ListOfSituation)\n",
    "\n",
    "Situation_count = call_Situation.value_counts()\n",
    "\n",
    "# Get the top 10 most frequent situations\n",
    "top10 = Situation_count.nlargest(10)\n",
    "print(\"The top 1o list\\n\",top10)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 Primary Situation\n",
    "\n",
    "This code generates a horizontal bar chart to show the top 10 situations in the dataset. It uses the Matplotlib library to create the chart and seaborn to define the color palette.\n",
    "\n",
    "The first step is to define the size of the chart, then the code creates a color map. The top 10 situations are plotted using a horizontal bar chart with the defined color palette. The chart's title and axis labels are defined, and grid lines are added.\n",
    "\n",
    "The x-tick labels are rotated and adjusted in font size. The legend is added to the chart, and the spacing is adjusted. The chart is converted to HTML and saved to a file named \"Top_10_plot.html\". Finally, the chart is displayed using the \"plt.show()\" function.\n",
    "\n",
    "We continue our data analysis and let's explore our plot. Out of this plot we can see which situations are most commonly reported, and it could help us understand allocation and emergency response strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#\n",
    "# Author: \n",
    "# Salim Omar\n",
    "#\n",
    "##\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "# define a color map\n",
    "cmap = sns.color_palette(\"ch:s=-.2,r=.6\")[::-1]\n",
    "\n",
    "# plot the top 10 situations with the colormap\n",
    "top10.plot(kind='bar', color=cmap, ax=ax)\n",
    "\n",
    "# set the chart title and axis labels\n",
    "plt.title('Top 10 Situations ', fontsize=20)\n",
    "plt.xlabel('Call Type', fontsize=16)\n",
    "plt.ylabel('count', fontsize=16)\n",
    "\n",
    "# add grid lines\n",
    "ax.grid(True)\n",
    "\n",
    "# adjust x-tick labels rotation and font size\n",
    "plt.xticks(rotation=15, fontsize=10)\n",
    "new_xticklabels = [label.get_text()[3:] for label in ax.get_xticklabels()]\n",
    "ax.set_xticklabels(new_xticklabels)\n",
    "\n",
    "# add legend\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "\n",
    "# adjust spacing\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "\n",
    "# convert to HTML and save\n",
    "html = mpld3.fig_to_html(fig)\n",
    "with open('Top_10_plot.html', 'w') as f:\n",
    "    f.write(html)\n",
    "\n",
    "# display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bokeh plot for top 10 Primary Situation and neighborhood_district\n",
    "\n",
    "First, this code loads some data into a DataFrame and selects the top 10 situations based on their frequency count. Then, it filters the data to only include the rows where the incident year is 2022 and the primary situation is one of the top 10 situations.\n",
    "\n",
    "Next, it groups the data by primary situation and neighborhood district and calculates the count for each group. It saves the resulting DataFrame to a CSV file. It also calculates the total count for each primary situation.\n",
    "\n",
    "Then, it pivots the DataFrame to create a new DataFrame that has neighborhood districts as rows and primary situations as columns, with the count of each primary situation for each neighborhood district as the value.\n",
    "\n",
    "Finally, it uses Bokeh to create a bar chart for each primary situation, with the neighborhoods on the x-axis and the count on the y-axis. It then adds each bar to the chart and stores the resulting chart elements in a dictionary.First, this code loads some data into a DataFrame and selects the top 10 situations based on their frequency count. Then, it filters the data to only include the rows where the incident year is 2022 and the primary situation is one of the top 10 situations.\n",
    "\n",
    "Next, it groups the data by primary situation and neighborhood district and calculates the count for each group. It saves the resulting DataFrame to a CSV file. It also calculates the total count for each primary situation.\n",
    "\n",
    "Then, it pivots the DataFrame to create a new DataFrame that has neighborhood districts as rows and primary situations as columns, with the count of each primary situation for each neighborhood district as the value.\n",
    "\n",
    "Finally, it uses Bokeh to create a bar chart for each primary situation, with the neighborhoods on the x-axis and the count on the y-axis. It then adds each bar to the chart and stores the resulting chart elements in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#\n",
    "# Author: \n",
    "# Salim Omar\n",
    "#\n",
    "##\n",
    "\n",
    "\n",
    "# get a list of the top 10\n",
    "top10_situations = top10.index.tolist()\n",
    "#print(top10_situations)\n",
    "#print(df['Primary Situation'])\n",
    "top10_df = top10_df[top10_df['Incident year'] == 2022]\n",
    "top10_df  = top10_df[top10_df['Primary Situation'].isin(top10_situations)]\n",
    "print(df['Primary Situation'])\n",
    "neighborhood_Primary_Situation = top10_df.groupby(['Primary Situation','neighborhood_district']).size().reset_index(name='count')\n",
    "#print(Battalion_Primary_Situation)\n",
    "\n",
    "neighborhood_Primary_Situation.to_csv(\"neighborhood_Primary_Situation.csv\")\n",
    "# calculate the total count for each neighborhood\n",
    "neighborhood_Primary_counts = top10_df.groupby(['Primary Situation']).size().reset_index(name='total_count')\n",
    "#print(neighborhood_Primary_counts)\n",
    "\n",
    "# merge the two dataframes to get the total count for each row\n",
    "neighborhood_Primary_Situation = pd.merge(neighborhood_Primary_Situation, neighborhood_Primary_counts, on='Primary Situation')\n",
    "#print(neighborhood_Primary_Situation)\n",
    "\n",
    "\n",
    "#calculate the count pr ituation pr neighborhood\n",
    "neighborhood_Primary_Situation['count_pr_Situation_pr_neighborhood'] = neighborhood_Primary_Situation['count'] \n",
    "\n",
    "\n",
    "columns = ['Primary Situation', 'neighborhood_district', 'count_pr_Situation_pr_neighborhood']\n",
    "focusData =  pd.DataFrame(neighborhood_Primary_Situation, columns=columns)\n",
    "#print(focusData)\n",
    "\n",
    "# Pivot the dataframe\n",
    "pivoted_focusData = focusData.pivot_table(index='neighborhood_district', columns='Primary Situation', values='count_pr_Situation_pr_neighborhood')\n",
    "\n",
    "# Display the pivoted dataframe\n",
    "#print(pivoted_focusData)\n",
    "\n",
    "\n",
    "source = ColumnDataSource(data=pivoted_focusData)\n",
    "## it is a standard way to convert your df to bokeh\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "# Define a figure with title and axis labels\n",
    "p = figure(x_range=source.data['neighborhood_district'], title=\"counts for call Situation pr neighborhood\",x_axis_label='neighborhood',width =1800)\n",
    "colo = Category10[10]\n",
    "p.xaxis.major_label_orientation = 1.2\n",
    "#3. Now we are going to add the bars. In order to do so, we will use vbar (see the guide for help):\n",
    "bar ={} # to store vbars\n",
    "items=[]\n",
    "\n",
    "\n",
    "### here we will do a for loop:\n",
    "for indx,Situation  in enumerate(pivoted_focusData.columns):\n",
    "    bar[Situation] =p.vbar(x='neighborhood_district', \n",
    "    top=Situation ,\n",
    "    source=source,\n",
    "    muted=True, \n",
    "    muted_alpha=0.05,\n",
    "    fill_alpha=1.9,\n",
    "    color=colo[indx],\n",
    "    width=0.7)\n",
    "    items.append((Situation, [bar[Situation]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below allows us to visualize the count for each call situation in each area with a histogram generated with the Bokeh library.\n",
    "The chart provides a clear and easy-to-understand view of the data and helps us identify any patterns or trends in the distribution of calls by area and call situation.\n",
    "In this diagram, by clicking on a type of call on the left side, we can see in which areas which types of situations occur more than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#\n",
    "# Author: \n",
    "# Salim Omar\n",
    "#\n",
    "##\n",
    "# The last thing to do is to make legend interactive and display the figure:\n",
    "legend = Legend(items=items)\n",
    "p.add_layout(legend, 'left')\n",
    "p.legend.click_policy = \"mute\"\n",
    "output_file('bokeh_Situation_pr_neighborhood.html')\n",
    "show(p)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Map to show the distribution for 4 Primary Situations in different Neighborhood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code selects a subset of data from a dataframe df containing fire incident data for the month of December 2022. It creates a new dataframe df_2022 with only the important columns, and drops the others. It then replaces the values in the focuse_Situation_by_number column with more descriptive labels, and keeps only the rows with specific situations of interest. Finally, it converts the point column to latitude and longitude coordinates and stores them in new columns lat and lon. The code also prints the unique values of the focuse_Situation_by_number column and the length of the resulting df_2022 dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#\n",
    "# Author: \n",
    "# Salim Omar\n",
    "#\n",
    "##\n",
    "\n",
    "# dataset for map\n",
    "df_2022 = df[(df['Incident Date'] >= '2022-12-01') &\n",
    "            (df['Incident Date'] <= '2022-12-31')]\n",
    "df_2022.head()\n",
    "# len(df_2022)\n",
    "focuse_Situation2 = df_2022['Primary Situation']\n",
    "ListOfSituation2 = focuse_Situation2.unique()\n",
    "#print(ListOfSituation2)\n",
    "len(ListOfSituation2)\n",
    "\n",
    "focuse_Situation = df_2022['focuse_Situation_by_number']\n",
    "ListOfSituation = focuse_Situation.unique()\n",
    "#print(ListOfSituation)\n",
    "from shapely import wkt\n",
    "# df_2022.loc[df_2022['focuse_Situation_by_number'].str.startswith('1'), 'focuse_Situation_by_number'] = 'Fire/explosion'\n",
    "# df_2022.loc[df_2022['focuse_Situation_by_number'].str.startswith('5'), 'focuse_Situation_by_number'] = 'Public service'\n",
    "# df_2022.loc[df_2022['focuse_Situation_by_number'].str.startswith('7'), 'focuse_Situation_by_number'] = 'Alarm'\n",
    "# df_2022['focuse_Situation_by_number'].replace(['322','324'], 'Motor vehicle accident', inplace=True)\n",
    "# df_2022['focuse_Situation_by_number'].replace(['311'], 'Medical assist', inplace=True)\n",
    "# df_2022['focuse_Situation_by_number'].replace(['700'], 'False alarm/call', inplace=True)\n",
    "# df_2022['focuse_Situation_by_number'].replace(['322'], 'Motor vehicle accident with injuries', inplace=True)\n",
    "# df_2022['focuse_Situation_by_number'].replace(['311'], 'Medical assist', inplace=True)\n",
    "df_2022 = df_2022[df_2022['focuse_Situation_by_number'].isin(['111', '700', '113','150'])]\n",
    "df_2022['focuse_Situation_by_number'].replace(['150'], 'Outside rubbish fire', inplace=True)\n",
    "df_2022['focuse_Situation_by_number'].replace(['111'], 'Building fire', inplace=True)\n",
    "df_2022['focuse_Situation_by_number'].replace(['700'], 'False alarm/call', inplace=True)\n",
    "df_2022['focuse_Situation_by_number'].replace(['113'], 'Cooking fire', inplace=True)\n",
    "\n",
    "# keep the important coulmn and drop other \n",
    "df_2022 = df_2022.loc[:, ['ID', 'point', 'Incident year', 'focuse_Situation_by_number']]\n",
    "\n",
    "\n",
    "df_2022['point'] = df_2022['point'].apply(wkt.loads)\n",
    "df_2022['lon'] = df_2022['point'].apply(lambda p: p.x)\n",
    "df_2022['lat'] = df_2022['point'].apply(lambda p: p.y)\n",
    "\n",
    "focuse_Situation = df_2022['focuse_Situation_by_number']\n",
    "ListOfSituation = focuse_Situation.unique()\n",
    "print(ListOfSituation)\n",
    "\n",
    "df_2022.head()\n",
    "len(df_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#\n",
    "# Author: \n",
    "# Salim Omar\n",
    "#\n",
    "##\n",
    "\n",
    "with open('../geo_map_data/Planning Neighborhood Groups Map.geojson', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data[\"features\"][0]\n",
    "\n",
    "\n",
    "# Create a dictionary to map focuse_Situation_by_number values to colors\n",
    "situation_color_dict = {\n",
    "    # 'Fire/explosion': 'green',\n",
    "    'Outside rubbish fire': 'orange',\n",
    "    'False alarm/call': 'purple',\n",
    "    'Cooking fire': 'blue',\n",
    "    'Building fire': 'red'\n",
    "}\n",
    "\n",
    "color_icon_dict = {\n",
    "    'red': 'fa-fire',\n",
    "    # 'green': 'fa-aulance',\n",
    "    'blue': 'fa-building',\n",
    "    'orange': 'fa-free-code-camp',\n",
    "    'purple': 'fa-bell'\n",
    "}\n",
    "\n",
    "\n",
    "def random_color(fea):\n",
    "    return {\n",
    "        'color': 'black',\n",
    "        'fillColor': random.choice(['red', 'yellow', 'green', 'orange']),\n",
    "    }\n",
    "\n",
    "\n",
    "my_map = Map(center=(37.7749, -122.4194), zoom=12,\n",
    "    layout={'height': '600px', 'width': '100%'})\n",
    "\n",
    "\n",
    "# Add GeoJSON layer to the map\n",
    "geojson_layer = GeoJSON(\n",
    "    data=data,\n",
    "    style={\n",
    "        'color': 'gray',\n",
    "        'weight': 3,\n",
    "        'fillOpacity': 0.2\n",
    "\n",
    "    },\n",
    "    hover_style={\n",
    "        'color': 'white', 'dashArray': '0', 'fillOpacity': 0.4,\n",
    "    },\n",
    "    style_callback=random_color,\n",
    "    name='Neighborhoods',\n",
    ")\n",
    "\n",
    "\n",
    "my_map.add_layer(geojson_layer)\n",
    "\n",
    "# Add markers to the map for each incident in the data\n",
    "for index, row in df_2022.iterrows():\n",
    "    location = (row['lat'], row['lon'])\n",
    "    marker_color = situation_color_dict[row['focuse_Situation_by_number']]\n",
    "    marker = Marker(location=location, draggable=False,\n",
    "                    title=row['focuse_Situation_by_number'])\n",
    "    marker.icon = AwesomeIcon(\n",
    "        name=color_icon_dict[marker_color], marker_color=marker_color, icon_color='black')\n",
    "    my_map.add_layer(marker)\n",
    "\n",
    "\n",
    "my_map.add_control(FullScreenControl())\n",
    "\n",
    "# Display the map\n",
    "my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#\n",
    "# Author: \n",
    "# Salim Omar\n",
    "#\n",
    "##\n",
    "\n",
    "import folium\n",
    "from folium.plugins import Fullscreen\n",
    "\n",
    "# create a Folium map object from the ipyleaflet map\n",
    "m = folium.Map(location=my_map.center,\n",
    "            zoom_start=my_map.zoom, control_scale=True)\n",
    "\n",
    "color_icon_dict2 = {\n",
    "    'red': 'fire',\n",
    "    'blue': 'cloud',\n",
    "    'orange': 'trash',\n",
    "    'purple': 'bell'\n",
    "}\n",
    "\n",
    "# Add GeoJSON layer to the map\n",
    "geojson_layer = folium.GeoJson(\n",
    "    data=data,\n",
    "    style_function=lambda features: {\n",
    "        'color': 'gray',\n",
    "        'weight': 3,\n",
    "        'fillOpacity': 0.2\n",
    "    },\n",
    "    highlight_function=lambda x: {'fillColor': random.choice(\n",
    "        ['red', 'yellow', 'green', 'orange', 'blue'])},\n",
    "    name='Neighborhoods',\n",
    "\n",
    ")\n",
    "geojson_layer.add_to(m)\n",
    "\n",
    "# Add markers to the map for each incident in the data\n",
    "for index, row in df_2022.iterrows():\n",
    "    location = (row['lat'], row['lon'])\n",
    "    marker_color = situation_color_dict[row['focuse_Situation_by_number']]\n",
    "    icon = folium.Icon(icon=color_icon_dict2[marker_color], color=marker_color,icon_color='black')\n",
    "    marker = folium.Marker(location=location, draggable=False,\n",
    "                        title=row['focuse_Situation_by_number'], icon=icon)\n",
    "    marker.add_to(m)\n",
    "\n",
    "# add Fullscreen control to the map\n",
    "Fullscreen().add_to(m)\n",
    "\n",
    "# save the map as an HTML file\n",
    "\n",
    "m.save('Situation_map.html')\n",
    "m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pie chart/donut chart for top 8 heat sources fire-causing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#\n",
    "# Author: \n",
    "# Salim Omar\n",
    "#\n",
    "##\n",
    "\n",
    "# Drop rows with missing data and unwanted values\n",
    "df.dropna(subset=['Heat Source'], inplace=True)\n",
    "df = df[~df['Heat Source'].isin(['UU Undetermined', 'UU - Undetermined', '-'])]\n",
    "\n",
    "# Get the top 8 Heat Sources\n",
    "top_heat_sources1 =df['Heat Source'].str[3:]\n",
    "top_heat_sources1 = top_heat_sources1.str.replace('- ', '')\n",
    "top_heat_sources = top_heat_sources1.value_counts().nlargest(8)\n",
    "print(top_heat_sources)\n",
    "\n",
    "# Create a figure and axis with equal aspect ratio\n",
    "fig, ax = plt.subplots(figsize=(10,8), subplot_kw=dict(aspect=\"equal\"))\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to format the autopct labels with percentage and absolute count\n",
    "def func(pct, allvals):\n",
    "    absolute = int(np.round(pct/100.*np.sum(allvals)))\n",
    "    return f\"{pct:.1f} %\\n \"\n",
    "\n",
    "# Create the pie chart\n",
    "wedges, texts, autotexts = ax.pie(top_heat_sources.values, autopct=lambda pct: func(pct, top_heat_sources.values),\n",
    "                                textprops=dict(color=\"w\"))\n",
    "# Add legend with the top 8 heat sources and adjust font size\n",
    "ax.legend(wedges, top_heat_sources.index,\n",
    "        title=\"Top 8 heat sources\",\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1, 0, 0.5, 1),\n",
    "        prop={'size': 14}\n",
    "        )\n",
    "\n",
    "\n",
    "# Adjust font size and color for the autopct labels\n",
    "plt.setp(autotexts, size=13, weight=\"bold\", color=\"black\")\n",
    "# Add title to the plot\n",
    "ax.set_title(\"Top 8 heat sources\",weight=\"bold\",size=15)\n",
    "\n",
    "# convert to HTML and save\n",
    "html = mpld3.fig_to_html(fig)\n",
    "with open('heat_sources_plot.html', 'w') as f:\n",
    "    f.write(html)\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bar chart"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates the time difference between the \"Arrival DtTm\" column and the \"Alarm DtTm\" column in a pandas DataFrame, and then saves the result in a new column called \"Arrive time_minutes\". The time difference is calculated in minutes and represents the time it took for responders to arrive at the incident after the alarm was triggered. The df.head() line would display the first few rows of the DataFrame, including the new \"Arrive time_minutes\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Author- s172858\n",
    "# Ali Dadayev \n",
    "\n",
    "\n",
    "\n",
    "# It calculates the time difference between the \"Arrival DtTm\" column and the \"Alarm DtTm\" column and saves it in a new column called \"Arrive time_minutes\".\n",
    "df['Arrive time_minutes'] = (df['Arrival DtTm'] - df['Alarm DtTm'])\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates the mean value of the \"Arrive time_minutes\" column in the DataFrame df, which was previously calculated by subtracting the \"Alarm DtTm\" column from the \"Arrival DtTm\" column. The resulting value represents the average time it takes for emergency responders to arrive at the scene after an alarm is triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Author- s172858\n",
    "# Ali Dadayev \n",
    "\n",
    "\n",
    "# This line calculates the average (mean) value of the \"Arrive time_minutes\" column of the pandas DataFrame df, and stores the result in the variable average_arrival_time.\n",
    "average_arrival_time = df['Arrive time_minutes'].mean()\n",
    "\n",
    "# This line prints out a message to the console that includes the average arrival time.\n",
    "# print(\"The average arrival time is:\", average_arrival_time)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code computes the arrival time in minutes for each incident by subtracting the \"Alarm DtTm\" column from the \"Arrival DtTm\" column, converts the time difference to minutes and rounds the result to two decimal points. It then groups the incidents by their respective neighborhood districts and calculates the average arrival time in minutes for each district. The result is saved in a new column called \"Arrive time_minutes\" and then grouped by neighborhood_district to get the average arrival time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Author- s172858\n",
    "# Ali Dadayev \n",
    "\n",
    "\n",
    "#This code creates a new column in df called \"Arrive time_minutes\" that contains the difference between the \"Arrival DtTm\" and \"Alarm DtTm\" columns of df.\n",
    "df['Arrive time_minutes'] = ((df['Arrival DtTm'] - df['Alarm DtTm']).dt.total_seconds() / 60.0).round(2)\n",
    "\n",
    "\n",
    "# Convert to minutes and add a new column\n",
    "\n",
    "#df = df[df['Battalion'] != 'B99']\n",
    "avg_arrival_time_by_neighborhood = df.groupby('neighborhood_district')['Arrive time_minutes'].mean().round(2)\n",
    "\n",
    "#print(avg_arrival_time_by_battalion)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet filters a DataFrame to only include incidents that occurred between December 1st, 2022 and December 31st, 2022 and have one of the four selected primary situations. It then calculates the average arrival time for each neighborhood district and creates a bar chart displaying the results, with the color of the bars indicating the average arrival time. A red dashed line is also added to indicate the overall average arrival time across all neighborhood districts. The resulting plot is saved as an HTML file and also displayed in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Author-s172858\n",
    "# Ali Dadayev \n",
    "# Filter for the year 2022\n",
    "\n",
    "# we taje the date that match the map from Salim Part \n",
    "df = df[(df['Incident Date'] >= '2022-12-01') &\n",
    "            (df['Incident Date'] <= '2022-12-31')]\n",
    "\n",
    "# we just incloud the primry situation that are showing on the map \n",
    "df = df[df['focuse_Situation_by_number'].isin(['111', '700', '113','150'])]\n",
    "\n",
    "# Convert to minutes and add a new column\n",
    "df['Arrive time_minutes'] = ((df['Arrival DtTm'] - df['Alarm DtTm']).dt.total_seconds() / 60.0).round(2)\n",
    "\n",
    "#df = df[df['Battalion'] != 'B99']\n",
    "avg_arrival_time_by_battalion = df.groupby('neighborhood_district')['Arrive time_minutes'].mean().round(2).reset_index()\n",
    "avg_arrival_time_by_battalion = avg_arrival_time_by_battalion.sort_values('Arrive time_minutes')\n",
    "mean_arrival_time = avg_arrival_time_by_battalion['Arrive time_minutes'].mean()\n",
    "# Create a bar chart with color gradient\n",
    "data = [go.Bar(\n",
    "            x=avg_arrival_time_by_battalion['neighborhood_district'],\n",
    "            y=avg_arrival_time_by_battalion['Arrive time_minutes'],\n",
    "            marker=dict(color=avg_arrival_time_by_battalion['Arrive time_minutes'],\n",
    "                        colorscale='Reds',\n",
    "                        cmin=1,   # set the minimum color value\n",
    "                        cmax=9,   # set the maximum color value\n",
    "                        reversescale=False\n",
    "                        ),\n",
    "            text=avg_arrival_time_by_battalion['Arrive time_minutes'],\n",
    "            textposition='auto'\n",
    "        )]\n",
    "\n",
    "\n",
    "\n",
    "# Set layout options\n",
    "layout = go.Layout(\n",
    "    title='Average Arrival Time by neighborhood (12/2022 --> 12/2022)',\n",
    "    xaxis=dict(title='neighborhood'),\n",
    "    yaxis=dict(title='Average Arrival Time (Minutes)', range=[1, 7]),\n",
    "    hovermode='closest',\n",
    "    width=1300,\n",
    "    height=800,\n",
    "    \n",
    "    shapes=[dict(type='line', x0=-0.5, y0=mean_arrival_time, x1=len(avg_arrival_time_by_battalion)-0.5, y1=mean_arrival_time,\n",
    "                 line=dict(color='red', width=2, dash='dash'))]\n",
    ")\n",
    "\n",
    "# Create the figure and save to an HTML file\n",
    "fig = go.Figure(data=data, layout=layout )\n",
    "pyo.plot(fig, filename='plot_for_neighborhood.html')\n",
    "\n",
    "\n",
    "\n",
    "# Display the plot in the notebook\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Line chart "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates and visualizes the average arrival time of fire department battalions to incidents over the years. It first calculates the arrival time in minutes by subtracting the alarm time from the arrival time and converts it into minutes. Then, it groups the data by battalion and year, calculates the mean arrival time for each group, and creates a line plot for each battalion using Plotly. It also adds interactivity to the plot using mplcursors. Finally, it saves the plot to an HTML file and displays it in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Author- s172858\n",
    "# Ali Dadayev \n",
    "\n",
    "\n",
    "# Convert to minutes and add a new column\n",
    "df['Arrive time_minutes'] = ((df['Arrival DtTm'] - df['Alarm DtTm']).dt.total_seconds() / 60.0).round(2)\n",
    "\n",
    "# Group by battalion and year\n",
    "df_grouped = df.groupby(['Battalion', 'Incident year'])['Arrive time_minutes'].mean().reset_index()\n",
    "\n",
    "# Create a line plot for each battalion using Plotly\n",
    "fig = go.Figure()\n",
    "for battalion in df_grouped['Battalion'].unique():\n",
    "    data = df_grouped[df_grouped['Battalion'] == battalion]\n",
    "    fig.add_trace(go.Scatter(x=data['Incident year'], y=data['Arrive time_minutes'], name=battalion, line=dict(width=2)))\n",
    "\n",
    "# Set layout for the plot\n",
    "fig.update_layout(\n",
    "    title=\"Average Arrival Time by Battalion and Year\",\n",
    "    xaxis_title=\"Year\",\n",
    "    yaxis_title=\"Average Arrival Time (Minutes)\",\n",
    "    font=dict(\n",
    "        family=\"Arial\",\n",
    "        size=16,\n",
    "        color=\"#7f7f7f\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        title=\"Battalion\",\n",
    "        font=dict(\n",
    "            family=\"Arial\",\n",
    "            size=12,\n",
    "            color=\"#7f7f7f\"\n",
    "        ),\n",
    "        yanchor=\"top\",\n",
    "        y=1,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    ),\n",
    "    plot_bgcolor=\"#f2f2f2\",\n",
    "    xaxis=dict(\n",
    "        tickmode='linear',\n",
    "        tick0=2003,\n",
    "        dtick=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add interactivity to the plot using mplcursors\n",
    "annotations = [f\"{battalion}\\nYear: {int(data['Incident year'])}\\nAvg. Arrival Time: {data['Arrive time_minutes']:.2f} minutes\"\n",
    "               for battalion, data in df_grouped[['Battalion', 'Incident year', 'Arrive time_minutes']].iterrows()]\n",
    "cursor = mplcursors.cursor(hover=True)\n",
    "cursor.connect(\"add\", lambda sel: sel.annotation.set_text(annotations[sel.target.index]))\n",
    "\n",
    "# Save the plot to an HTML file and display it in the browser\n",
    "pyo.plot(fig, filename='battalion_arrival_time.html', auto_open=True)\n",
    "\n",
    "# Display the plot in the notebook\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Polar bar chart."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates a polar bar plot showing the hourly count of alarms in the year 2022. The first line converts the 'Alarm DtTm' column in the DataFrame 'df' to a datetime format using Pandas' to_datetime() method. The second line filters the DataFrame for the year 2022 and stores it in a new variable called 'df2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author- s172858\n",
    "# Ali Dadayev \n",
    "\n",
    "\n",
    "# Convert the alarm datetime column to datetime\n",
    "df['Alarm DtTm'] = pd.to_datetime(df['Alarm DtTm'])\n",
    "\n",
    "# Filter for the year 2022\n",
    "df2 = df[df['Alarm DtTm'].dt.year == 2022]\n",
    "\n",
    "# Group by hour and count number of alarms\n",
    "hour_counts = df2.groupby(df2['Alarm DtTm'].dt.hour).size().reset_index(name='counts')\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'}, figsize=(10,10))\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(hour_counts)))\n",
    "\n",
    "bars = ax.bar(hour_counts['Alarm DtTm'] * 2 * np.pi / 24, hour_counts['counts'], \n",
    "              width=2*np.pi/24, align='edge', color=colors, alpha=0.8)\n",
    "\n",
    "hours = np.arange(0, 24)\n",
    "tick_labels = ['{}:00'.format(h) for h in range(24)]\n",
    "ax.set_xticks(np.linspace(0, 2*np.pi, 24, endpoint=False))\n",
    "ax.set_xticklabels(tick_labels, fontsize=12, color='black', fontweight='bold')\n",
    "ax.set_title('Alarm Hourly Counts in 2022', fontsize=20, pad=25, fontweight='bold')\n",
    "\n",
    "\n",
    "# Set the starting angle and direction\n",
    "ax.set_theta_offset(np.pi/2)\n",
    "ax.set_theta_direction(-1)\n",
    "\n",
    "# Customize the grid and background\n",
    "ax.grid(color='gray', alpha=0.2)\n",
    "ax.set_facecolor('whitesmoke')\n",
    "\n",
    "# Remove unnecessary borders\n",
    "ax.spines['polar'].set_visible(False)\n",
    "ax.spines['start'].set_visible(False)\n",
    "ax.spines['end'].set_visible(False)\n",
    "ax.spines['inner'].set_visible(False)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates a map showing the locations of fire stations in a city, specifically those under the jurisdiction of the Fire Department. The map is created using the Python library Folium and is centered on the city of San Francisco. Each fire station is represented by a marker on the map, and clicking on a marker displays the common name of the corresponding fire station in a popup.\n",
    "\n",
    "This visualization can be useful for identifying the locations of fire stations within a city and their proximity to different areas. It may also be useful for emergency response planning or for residents to locate the nearest fire station in case of an emergency. The resulting HTML file can be opened in a web browser for further exploration and interaction with the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Author Thomas Arildtoft - S193564\n",
    "\n",
    "# Load the data\n",
    "fire_stations = pd.read_csv('../geo_map_data/City-owned_Facilities_-_Fire_and_Police.csv')\n",
    "\n",
    "# Filter the data to only include fire stations with \"Fire Department\" in the jurisdiction\n",
    "san_francisco_fire_stations = fire_stations[fire_stations['jurisdiction'] == 'Fire Department']\n",
    "\n",
    "# Create a folium map centered on San Francisco\n",
    "m = folium.Map(location=[37.773972, -122.431297], zoom_start=13)\n",
    "\n",
    "# Add markers for each fire station\n",
    "for index, row in san_francisco_fire_stations.iterrows():\n",
    "        folium.Marker(location=[row['latitude'], row['longitude']], popup=row['common_name']).add_to(m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('Fire_stations_map.html')\n",
    "m\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This visualization shows the total number of alarms for all battalions per year in a stacked bar chart. The data is filtered to include only incidents from specific battalions that occurred between April 1st, 2010 and April 1st, 2023. The data is then grouped by battalion and year, and the number of incidents for each battalion in each year is counted. The resulting table is pivoted so that each battalion is a row and each year is a column. The stacked bar chart shows the total number of alarms for each year, with each battalion's contribution represented by a different color. The legend is placed outside the plot area for clarity. Hover effects are added to the chart, so that when the mouse is over a specific area, the battalion, year, and count of alarms for that area are displayed. Finally, the plot is converted to HTML and saved to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author Thomas Arildtoft - S193564\n",
    "\n",
    "# Filter the data by Battalion and Incident Date\n",
    "df_filtered = df[(df['Battalion'].isin(['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B10'])) &\n",
    "                 (df['Incident Date'] >= '2010-04-01') &\n",
    "                 (df['Incident Date'] <= '2023-04-01')]\n",
    "\n",
    "# Create a new column with the year of the incident\n",
    "df_filtered['Year'] = pd.DatetimeIndex(df_filtered['Incident Date']).year\n",
    "\n",
    "# Group the data by Battalion and Year and count the number of incidents\n",
    "df_grouped = df_filtered.groupby(['Battalion', 'Year'])['Incident Number'].count().reset_index()\n",
    "\n",
    "# Pivot the data to create a table with Battalion as rows and Year as columns\n",
    "df_pivoted = df_grouped.pivot(index='Year', columns='Battalion', values='Incident Number')\n",
    "\n",
    "# Create a stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "df_pivoted.plot(kind='bar', stacked=True, ax=ax)\n",
    "\n",
    "# Set the title and axis labels\n",
    "ax.set_title('Total number of Alarms for all Battalions per. year')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Number of Alarms')\n",
    "\n",
    "# Move the legend outside the plot area\n",
    "ax.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0)\n",
    "\n",
    "# Add hover effects to the plot\n",
    "cursor = mplcursors.cursor(ax, hover=True)\n",
    "@cursor.connect('add')\n",
    "def on_add(sel):\n",
    "    battalion = sel.artist.get_label()\n",
    "    year = sel.target[0]\n",
    "    count = df_pivoted.loc[year, battalion]\n",
    "    sel.annotation.set_text(f'Battalion: {battalion}\\nYear: {year}\\nCount: {count}')\n",
    "    sel.annotation.set_position((-20, 20))\n",
    "    sel.annotation.set_fontsize(12)\n",
    "    sel.annotation.set_fontstyle('italic')\n",
    "    sel.annotation.set_backgroundcolor('white')\n",
    "    sel.annotation.set_bbox({'boxstyle': 'round', 'edgecolor': 'gray', 'alpha': 0.7})\n",
    "\n",
    "# Convert the plot to HTML\n",
    "html_fig = mpld3.fig_to_html(fig)\n",
    "\n",
    "# Output the HTML\n",
    "with open('Total_Number_Of_alarms_Battalion.html', 'w') as f:\n",
    "    f.write(html_fig)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code analyzes the number of fire incidents that occurred in a city over a period of time. The data is visualized as a calendar plot where each square represents a day, and the color of the square indicates the number of fire incidents that occurred on that day. Darker colors indicate a higher number of incidents. The plot shows the data from April 1st, 2010 to April 1st, 2023.\n",
    "\n",
    "The plot helps to identify trends and patterns in the occurrence of fire incidents over time. By hovering over a specific date on the plot, the user can see the exact number of incidents that occurred on that date. The interactive HTML file created allows for further exploration of the data and interaction with the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author Thomas Arildtoft - S193564\n",
    "\n",
    "# Filter the fire incidents to keep only dates where incidents happened\n",
    "events_by_date = df.groupby('Incident Date').size()\n",
    "\n",
    "non_zero_dates = events_by_date[events_by_date > 0].index\n",
    "\n",
    "df_filtered = df[(df['Incident Date'].isin(non_zero_dates))]\n",
    "\n",
    "# Group the filtered DataFrame by date and count the number of incidents on each date\n",
    "counts = df_filtered.groupby('Incident Date').size()\n",
    "\n",
    "theRange = pd.date_range(start=\"2010-04-01\", end=\"2023-04-01\", freq='D')\n",
    "events = pd.Series(counts, index=theRange)\n",
    "\n",
    "# Set the colormap to 'cool'\n",
    "custom_cmap = plt.get_cmap('cool')\n",
    "fig, ax = calplot.calplot(events, cmap=custom_cmap)\n",
    "\n",
    "# Add a hover effect to show the value of each date\n",
    "cursor = mplcursors.cursor(ax, hover=True)\n",
    "@cursor.connect(\"add\")\n",
    "def on_add(sel):\n",
    "    index = sel.target.index\n",
    "    value = events.loc[index]\n",
    "    sel.annotation.set_text(f\"{index.strftime('%Y-%m-%d')}: {value}\")\n",
    "\n",
    "# Convert the plot to an interactive HTML format\n",
    "html_fig = mpld3.fig_to_html(fig)\n",
    "\n",
    "# Save the HTML file\n",
    "with open('calplot.html', 'w') as f:\n",
    "    f.write(html_fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code analyzes emergency response times for the top 10 most frequent emergency situations in a dataset. It calculates the average response time for each situation by subtracting the alarm time from the arrival time, in minutes. If the arrival time is missing, it is ignored for that situation. The code then creates a bar chart that shows the average response time for each of the top 10 emergency situations. The chart is interactive, allowing for exploration of the data and interaction with the chart. This information can help identify areas where improvements could be made to reduce response times for specific emergency situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author Thomas Arildtoft - S193564\n",
    "\n",
    "# Get top 10 most frequent unique values in 'Primary Situation' column\n",
    "top_situations = df['Primary Situation'].value_counts().nlargest(10).index.tolist()\n",
    "\n",
    "# Replace missing values in 'Arrival DtTm' column with 'Missing'\n",
    "df['Arrival DtTm'].fillna('Missing', inplace=True)\n",
    "\n",
    "# Calculate average response time for each situation\n",
    "avg_response_times = {}\n",
    "for situation in top_situations:\n",
    "    situation_rows = df[df['Primary Situation'] == situation]\n",
    "    response_times = []\n",
    "    for index, row in situation_rows.iterrows():\n",
    "        alarm_time = datetime.strptime(row['Alarm DtTm'].strftime('%Y-%m-%d %H:%M:%S'), '%Y-%m-%d %H:%M:%S')\n",
    "        if row['Arrival DtTm'] != 'Missing': # check for missing values\n",
    "            arrival_time = datetime.strptime(str(row['Arrival DtTm']), '%Y-%m-%d %H:%M:%S')\n",
    "            response_time = (arrival_time - alarm_time).total_seconds() / 60.0\n",
    "            response_times.append(response_time)\n",
    "    if response_times: # check if list is not empty\n",
    "        avg_response_time = sum(response_times) / len(response_times)\n",
    "        avg_response_times[situation] = avg_response_time\n",
    "\n",
    "# Plot bar chart of average response times for top 10 situations\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(len(avg_response_times)), list(avg_response_times.values()), align='center')\n",
    "ax.set_xticks(range(len(avg_response_times)))\n",
    "ax.set_xticklabels(list(avg_response_times.keys()), rotation='vertical')\n",
    "ax.set_ylabel('Average response time (minutes)')\n",
    "ax.set_title('Top 10 situations by frequency')\n",
    "\n",
    "# Create HTML file with interactive chart using mpld3\n",
    "html = mpld3.fig_to_html(fig)\n",
    "with open('response_times.html', 'w') as f:\n",
    "    f.write(html)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code produces a line chart of the average response times for the top 10 situations in a dataset from the years 2010 to 2023. The chart shows the trend of average response times over the years for each situation. Each line in the chart represents a situation and the x-axis represents years while the y-axis shows the average response time in minutes. The chart has a hover effect that displays the situation and its corresponding average response time when hovering over a point on the chart. The chart also has an information box that informs the viewer about the hover effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author Thomas Arildtoft - S193564\n",
    "\n",
    "# Convert 'Alarm DtTm' column to string\n",
    "df['Alarm DtTm'] = df['Alarm DtTm'].astype(str)\n",
    "\n",
    "# Get top 10 most frequent unique values in 'Primary Situation' column\n",
    "top_situations = df['Primary Situation'].value_counts().nlargest(10).index.tolist()\n",
    "\n",
    "# Replace missing values in 'Arrival DtTm' column with 'Missing'\n",
    "df['Arrival DtTm'].fillna('Missing', inplace=True)\n",
    "\n",
    "# Create dictionary to store data for each situation\n",
    "situation_data = {situation: [] for situation in top_situations}\n",
    "\n",
    "# Loop through each year from 2010 to 2023\n",
    "for year in range(2010, 2024):\n",
    "    year_rows = df[df['Alarm DtTm'].str.startswith(str(year))]\n",
    "    for situation in top_situations:\n",
    "        situation_rows = year_rows[year_rows['Primary Situation'] == situation]\n",
    "        response_times = []\n",
    "        for index, row in situation_rows.iterrows():\n",
    "            if row['Arrival DtTm'] != 'Missing': # check for missing values\n",
    "                alarm_time = datetime.strptime(row['Alarm DtTm'], '%Y-%m-%d %H:%M:%S')\n",
    "                arrival_time = pd.Timestamp.strftime(row['Arrival DtTm'], '%Y-%m-%d %H:%M:%S')\n",
    "                arrival_time = datetime.strptime(arrival_time, '%Y-%m-%d %H:%M:%S') # convert to datetime object\n",
    "                response_time = (arrival_time - alarm_time).total_seconds() / 60.0\n",
    "                response_times.append(response_time)\n",
    "        if response_times: # check if list is not empty\n",
    "            avg_response_time = sum(response_times) / len(response_times)\n",
    "            situation_data[situation].append(avg_response_time)\n",
    "        else:\n",
    "            situation_data[situation].append(None)\n",
    "\n",
    "# Plot line chart of average response times for top 10 situations by year\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for situation, data in situation_data.items():\n",
    "    ax.plot(range(2010, 2024), data, label=situation)\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "# Add x-axis label and tick labels\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_xticks(range(2010, 2024))\n",
    "ax.set_xticklabels(range(2010, 2024), rotation=90)\n",
    "\n",
    "# Add y-axis label\n",
    "ax.set_ylabel('Average response time (minutes)')\n",
    "\n",
    "# Add hover effects using mplcursors\n",
    "mplcursors.cursor(ax).connect('add', lambda sel: sel.annotation.set_text(f'{sel.artist.get_label()}: {sel.target[1]:.2f} minutes'))\n",
    "\n",
    "# Add information box outside the chart\n",
    "info_text = 'Hover over a point to see details'\n",
    "plt.text(1.05, 0.5, info_text, transform=ax.transAxes,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', edgecolor='gray'),\n",
    "         fontsize=12, ha='left', va='center')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a new Pandas DataFrame called neighborhoods_df that contains the unique values from the \"neighborhood_district\" column of an existing DataFrame called df. The pd.DataFrame() function is used to create the new DataFrame, passing in the array of unique values from the \"neighborhood_district\" column as the first argument, and specifying the name of the new column as \"neighborhood_district\" using the columns parameter.\n",
    "\n",
    "Finally, the print() function is used to display the new DataFrame to the console. This will output the unique values from the \"neighborhood_district\" column of the original DataFrame df, with each unique value appearing in its own row under the \"neighborhood_district\" column header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author Thomas Arildtoft - S193564\n",
    "\n",
    "# Create a new DataFrame with unique values in the \"neighborhood_district\" column\n",
    "neighborhoods_df = pd.DataFrame(df[\"neighborhood_district\"].unique(), columns=[\"neighborhood_district\"])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(neighborhoods_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code analyzes emergency incidents in a city in the year 2022 and shows the percentage of incidents with above-average response times by neighborhood. The data is visualized as a bar chart where each bar represents a neighborhood, and the height of the bar indicates the percentage of incidents with above-average response times. The chart also includes a red dashed line representing the average percentage of incidents above average response time.\n",
    "\n",
    "The chart helps to identify areas where improvements could be made to reduce response times and improve emergency services. The interactive HTML file created allows for further exploration of the data and interaction with the chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author Thomas Arildtoft - S193564\n",
    "\n",
    "# Filter for incidents in the year 2022 and with Primary Situation of \"500 Service Call, other\" or \"700 False alarm or false call, other\"\n",
    "filter_condition = (df[\"Alarm DtTm\"].astype(str).str.startswith(\"2022\")) & ((df[\"Primary Situation\"] == \"500 Service Call, other\") | (df[\"Primary Situation\"] == \"700 False alarm or false call, other\"))\n",
    "filtered_df = df.loc[filter_condition]\n",
    "\n",
    "# Calculate the response time for each incident and find the average\n",
    "response_time = pd.to_datetime(filtered_df[\"Arrival DtTm\"]) - pd.to_datetime(filtered_df[\"Alarm DtTm\"])\n",
    "average_response_time = response_time.mean()\n",
    "\n",
    "# Filter for incidents with response time above the average and find the percentage by neighborhood\n",
    "above_avg_condition = (response_time > average_response_time)\n",
    "above_avg_df = filtered_df.loc[above_avg_condition]\n",
    "percentage_by_neighborhood = above_avg_df[\"neighborhood_district\"].value_counts(normalize=True) * 100\n",
    "\n",
    "# Create a bar chart of the percentage of incidents with above average response time by neighborhood\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=percentage_by_neighborhood.index, y=percentage_by_neighborhood.values)\n",
    "plt.title(\"Percentage of Incidents with Above Average Response Time by Neighborhood (2022)\")\n",
    "plt.xlabel(\"Neighborhood\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylim(0, 10)\n",
    "plt.axhline(y=percentage_by_neighborhood.mean(), color=\"red\", linestyle=\"--\", label=\"Average Percentage\")\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as an HTML file using mpld3\n",
    "html_fig = mpld3.fig_to_html(plt.gcf())\n",
    "with open('Percentage_of_Incidents_Above_Average_Response_Time.html', 'w') as f:\n",
    "    f.write(html_fig)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is creating a heatmap using Folium to visualize the response time of emergency incidents in San Francisco by neighborhood.\n",
    "\n",
    "The dataset includes information on the incident location, type, and response time, among other things. The code first filters the dataset to include only incidents from 2022 with a primary situation of \"500 Service Call, other\" or \"700 False alarm or false call, other.\"\n",
    "\n",
    "The code then calculates the response time by subtracting the time the incident was reported from the time the emergency responders arrived at the scene. It filters the dataset to include only incidents with above-average response times and groups the data by neighborhood to calculate the average response time by neighborhood and the percentage of incidents with above-average response times by neighborhood.\n",
    "\n",
    "The code then merges this data with latitude and longitude information for each neighborhood and selects only the top five neighborhoods with the highest percentage of incidents with above-average response times.\n",
    "\n",
    "Finally, the code creates a Folium map centered on San Francisco and adds a heatmap layer to display the average response time by neighborhood. The heatmap shows the areas with the highest response times in darker shades, while the areas with lower response times are in lighter shades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author Thomas Arildtoft - S193564\n",
    "\n",
    "# Extract the coordinates from the \"point\" column and create separate columns for latitude and longitude\n",
    "df[\"coordinates\"] = df[\"point\"].str.replace(\"POINT \\(\", \"\").str.replace(\"\\)\", \"\")\n",
    "df[[\"longitude\", \"latitude\"]] = df[\"coordinates\"].str.split(expand=True)\n",
    "df = df.drop(columns=[\"coordinates\"])\n",
    "\n",
    "# Extract the incident year from the \"Incident Date\" column\n",
    "df[\"Incident Year\"] = pd.to_datetime(df[\"Incident Date\"]).dt.year\n",
    "\n",
    "# Filter the DataFrame to only include incidents from 2022 with primary situation of \"500 Service Call, other\" or \"700 False alarm or false call, other\"\n",
    "filtered_df = df[(df[\"Incident Year\"] == 2022) & (df[\"Primary Situation\"].isin([\"500 Service Call, other\", \"700 False alarm or false call, other\"]))]\n",
    "\n",
    "# Calculate the response time by subtracting \"Arrival DtTm\" from \"Alarm DtTm\"\n",
    "filtered_df[\"Response DtTm\"] = pd.to_datetime(filtered_df[\"Arrival DtTm\"]) - pd.to_datetime(filtered_df[\"Alarm DtTm\"])\n",
    "response_time = filtered_df[\"Response DtTm\"].dt.total_seconds() / 60\n",
    "\n",
    "# Calculate the average response time\n",
    "average_response_time = response_time.mean()\n",
    "\n",
    "# Filter the DataFrame to only include incidents with above average response time\n",
    "above_avg_condition = (response_time > average_response_time)\n",
    "above_avg_df = filtered_df.loc[above_avg_condition]\n",
    "response_time_by_neighborhood = above_avg_df.groupby(\"neighborhood_district\")[\"Response DtTm\"].mean().dt.total_seconds() / 60\n",
    "\n",
    "# Create a new DataFrame with the average response time by neighborhood\n",
    "neighborhoods_df = pd.DataFrame(response_time_by_neighborhood).reset_index()\n",
    "\n",
    "# Calculate the percentage of incidents with above average response time by neighborhood\n",
    "neighborhood_pct = (above_avg_df.groupby(\"neighborhood_district\")[\"Incident Number\"].count() / filtered_df.groupby(\"neighborhood_district\")[\"Incident Number\"].count() * 100).reset_index()\n",
    "neighborhood_pct = neighborhood_pct.rename(columns={\"Incident Number\": \"Pct Above Avg\"})\n",
    "\n",
    "# Merge the average response time by neighborhood DataFrame with the neighborhoods DataFrame and the neighborhood percentage DataFrame\n",
    "merged_df = pd.merge(neighborhoods_df, df[[\"neighborhood_district\", \"latitude\", \"longitude\"]].drop_duplicates(), on=\"neighborhood_district\")\n",
    "merged_df = pd.merge(merged_df, neighborhood_pct, on=\"neighborhood_district\")\n",
    "\n",
    "# Sort by percentage of above-average response times and select only the top 5 neighborhoods\n",
    "merged_df = merged_df.sort_values(by=\"Pct Above Avg\", ascending=False).head(5)\n",
    "\n",
    "# Create a folium map centered on San Francisco\n",
    "m = folium.Map(location=[37.7749, -122.4194], zoom_start=12)\n",
    "\n",
    "# Add a heatmap layer to the map using only the data for the top 5 neighborhoods\n",
    "HeatMap(data=merged_df[[\"latitude\", \"longitude\", \"Response DtTm\"]].values.tolist(), radius=10, blur=5).add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m.save('Response_time_map.html')\n",
    "m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text\n",
    "\n",
    "\n",
    "## Data Analysis\n",
    "\n",
    "   We used various data analysis techniques to gain insights into the fire incidents dataset. We performed exploratory data analysis to identify patterns and trends in the data, and created summary statistics and visualizations to communicate these insights to the user. We also used machine learning techniques to predict the cause of fire incidents based on other variables in the dataset.\n",
    "\n",
    "- ### Describe your data analysis and explain what you've learned about the dataset.\n",
    "\n",
    "- ### If relevant, talk about your machine-learning.\n",
    "\n",
    "## Genre\n",
    "\n",
    "- ### Which genre of data story did you use?\n",
    "\n",
    "   We used  \"Magazine Style\" genre and \"Annotated Graph / Map \" for our data story, as we focused on exploring a specific dataset with help of varity plots with some effects and animations to gain insights into the characteristics of fire incidents in San Francisco.\n",
    "- ### Which tools did you use from each of the 3 categories of Visual Narrative (Figure 7 in Segal and Heer). Why?\n",
    "\n",
    "    Visual Structuring:\n",
    "\n",
    "     - Consistent Visual Platform (everything hapens on same page mainly)\n",
    "\n",
    "     - Progress Bar (webpage has scrollbar)\n",
    "\n",
    "\n",
    "    Highlighting:\n",
    "\n",
    "     - Zooming and panning: We used zooming and panning to allow the user to focus on specific areas of the visualizations and explore them in more detail.\n",
    "        \n",
    "        \n",
    "    Transition Guidance:\n",
    "\n",
    "     - Familiar Objects (Several similar graph types)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- ### Which tools did you use from each of the 3 categories of Narrative Structure (Figure 7 in Segal and Heer). Why?\n",
    "\n",
    "    Ordering:\n",
    "\n",
    "     - Linear \n",
    "\n",
    "    Interactivity:\n",
    "\n",
    "     - Hover Highlighting / Details \n",
    "\n",
    "    Messaging:\n",
    "\n",
    "     - Introductory Text \n",
    "\n",
    "     - Multi-Messaging \n",
    "\n",
    "     - Captions / Headlines \n",
    "\n",
    "     - Summary \n",
    "\n",
    "     - Accompanying Article \n",
    "     \n",
    "         \n",
    "\n",
    "    We used these tools to help users understand the data better and to tell a compelling story about fire incidents in San Francisco. The data-driven sequences and interactive components help users to explore the data and draw their own conclusions, while the context and annotations provide additional information and insights.\n",
    "\n",
    "## Visualizations.\n",
    "\n",
    "- ### Explain the visualizations you've chosen.\n",
    "\n",
    "    - Bar plots - are a useful tool for visualizing and comparing differences between categories or groups of data. They are easy to interpret and can display a wide range of information, making them a versatile tool for data analysis and visualization.\n",
    "\n",
    "    - Line charts - are useful because they show changes in data over time, allow for easy comparison of multiple data sets, and are simple and easy to interpret.\n",
    "\n",
    "    - Bokeh charts - are useful for visualizing and exploring data in an interactive and dynamic way. With Bokeh, users can create a wide range of charts, including line, scatter, and bar charts, with customizable features such as axes, grids, and legends. Bokeh charts allow for exploration of large datasets with tools like zooming, panning, and hovering over data points to see more details. The interactive nature of Bokeh charts makes them particularly useful for data analysis and communication, as they allow users to uncover hidden patterns and trends in the data. Overall, Bokeh charts provide a powerful tool for data visualization and analysis that enables users to gain deeper insights into their data.\n",
    "\n",
    "    - Maps - are useful for data visualization because they allow for the representation of data in a spatial context. By plotting data on a map, viewers can see patterns and trends that may not be immediately apparent in a tabular or textual format. Maps can provide insights into geographic variations and distributions, allowing for easy identification of areas of high or low values. They are also useful for displaying data that is related to geographic locations, such as demographic information or environmental data. Overall, maps are a powerful tool for visualizing data and providing insights into spatial patterns and relationships.\n",
    "\n",
    "    - Donut charts - are a visually appealing and useful tool for comparing the proportions of different categories or groups within a dataset. They can display multiple categories in a single chart and can be customized to show additional information, making them a valuable tool for data visualization.\n",
    "\n",
    "    - Polar charts - are useful for displaying multiple variables at once and highlighting changes in data over time. They provide a visual representation of relationships between variables, making it easy to identify patterns and trends in the data.\n",
    "\n",
    "    - Pie charts - are useful for displaying proportions and percentages within a dataset in a clear and easy-to-understand way. They are particularly useful for showing data that can be divided into categories or groups.\n",
    "\n",
    "- ### Why are they right for the story you want to tell?\n",
    "\n",
    "    - Bar plots: Can be used to compare the frequency of different alarm types across neighborhoods or the average response times of different neighborhoods.\n",
    "    - Line charts: Can be used to visualize trends in response times over time within a neighborhood or across neighborhoods.\n",
    "    - Bokeh charts: Bokeh is a data visualization library that can create interactive and dynamic visualizations, such as scatter plots or heatmaps. These could be used to show the relationship between alarm types and response times across different neighborhoods.\n",
    "    - Maps: Can be used to provide a geographic context to the data, allowing viewers to see how alarm types and response times vary across different neighborhoods in San Francisco.\n",
    "    - Donut charts: Can be used to show the relative distribution of different alarm types in a single neighborhood or across neighborhoods.\n",
    "    - Polar charts: Similar to pie charts, polar charts can be used to show the relative distribution of different alarm types or response times across neighborhoods.\n",
    "    - Pie charts: Can be used to show the relative distribution of different alarm types in a single neighborhood or across neighborhoods.\n",
    "\n",
    "## Discussion. Think critically about your creation\n",
    "\n",
    "- ### What went well?\n",
    "    When looking at our plots, we succeeded showing our original idea behind them in a way we think would be informative for others to read, and hopefully understandable for people with little to no knowledge about the topic data visualization. We wanted to stick to what we have learned from our classes, but also try something new and different. Which we think we succeded with at least on our maps.\n",
    "\n",
    "- ### What is still missing? What could be improved?, Why?\n",
    "\n",
    "    We would have liked to provide more data on the differentiation of the neighborhoods, here we mean if neighborhoods with lower income had higher response times from the firestations, or the differentiation between neighborhoods with high and low crime rates, regarding alarm types that the fire departments would receive.\n",
    "\n",
    "    Our project is lacking in regards to direction, after having created mulitple charts and plots, we became aware of our project lacking a general direction, this meant we contacted Sune for clarification, which he gave us and we tried making the changes required.\n",
    "\n",
    "\n",
    "## Contributions. Who did what?\n",
    "\n",
    "- ### You should write (just briefly) which group member was the main responsible for which elements of the assignment. (I want you guys to understand every part of the assignment, but usually there is someone who took lead role on certain portions of the work. That's what you should explain).\n",
    "\n",
    "    We started out assigning certain plots for each group member, this has been documented inside the jupyter document with the author tags, otherwise we have labeled certain portiona of the plots and diagrams under \" Part \" this should otherwise make it obivous to identify each team members contribution.\n",
    "\n",
    "    Part 1 = Salim\n",
    "    Part 2 = Ali\n",
    "    Part 3 = Thomas\n",
    "\n",
    "    We then startede on the explainer notebook, this was an team effort since every single member was online on Discord, contributing to the assignment, so to specify who wrote what column inside this Notebook would be difficult.\n",
    "\n",
    "    The website, has contribution from us all towards HTML layout, CSS to the text written on the website\n",
    "\n",
    "## Make sure that you use references when they're needed and follow academic standards.\n",
    "\n",
    "Relevant links has been inserted in the text itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
